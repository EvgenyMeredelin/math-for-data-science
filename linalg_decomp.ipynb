{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [__Собственные векторы, собственные значения__](#toc1_1_1_)    \n",
    "- [__Спектральное разложение матрицы__](#toc1_1_2_)    \n",
    "- [__Рекомендательные системы__](#toc1_1_3_)    \n",
    "- [__Сингулярное разложение матрицы (SVD)__](#toc1_1_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <a id='toc1_1_1_'></a>[__Собственные векторы, собственные значения__](#toc0_)\n",
    "\n",
    "Пусть дано линейное преобразование $A : \\R^n \\rightarrow \\R^n$.\n",
    "\n",
    "__Собственным вектором__ преобразования $A$ называется ненулевой вектор $\\vec{v} \\in \\R^n$ такой, что $A\\vec{v} = \\lambda \\vec{v}$ для какого-нибудь числа $\\lambda \\in \\R$.\n",
    "<br>Иначе говоря, собственный вектор преобразования это такой вектор, действуя на который преобразование изменяет только его длину, но не направление.\n",
    "\n",
    "__Собственными числами__ преобразования $A$ называются корни уравнения $\\det(A - \\lambda\\text{E}) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-10,\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [1],\n",
       "   [1]])]),\n",
       " (-4,\n",
       "  1,\n",
       "  [Matrix([\n",
       "   [1/3],\n",
       "   [  1]])])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sym.Matrix([[-13, 3], [-9, -1]])\n",
    "M.eigenvects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.31622777],\n",
       "       [-0.70710678, -0.9486833 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[-13, 3], [-9, -1]])\n",
    "ev = np.linalg.eig(M)[1]\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.33333333])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev[0] / ev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEigenvec(M, v):\n",
    "    ev = np.linalg.eig(M)[1]\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm:\n",
    "        return np.any(np.isclose(np.abs(ev.T @ v / norm), 1))\n",
    "    return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея решения: Если тестируемый вектор __`v`__ - собственный вектор матрицы __`M`__, то он коллинеарен вектору, о котором доподлинно известно, что он собственный. Коллинеарность проверяется близостью к единице выраженного через скалярное произведение и норму вектора косинуса угла между тестируемым и собственным векторами - столбцами __`ev`__. Эти собственные векторы уже нормированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ev, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...и потому делить на их (единичные) длины не требуется. Тестируемый вектор может быть как сонаправленным собственному вектору, так и противоположно направленным - оба эти случая удовлетворяют условию собственности (ограничения на знак $\\lambda$ в определении нет). Поэтому с единицей сравнивается модуль косинуса. Меняя tolerance функции __`numpy.isclose`__, можно регулировать строгость сравнения, определяя, какие векторы считать \"достаточно коллинеарными\", а какой разлет уже критичным. Также учтено, что нулевым вектором не \"владеет\" никто. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isEigenvec(M, np.array([2, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isEigenvec(M, np.array([1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <a id='toc1_1_2_'></a>[__Спектральное разложение матрицы__](#toc0_)\n",
    "20/02/2023\n",
    "<br>Коварство матричных цепочек кроется в двух вещах.\n",
    "<br>Во-первых, матрицы могут ошибочно восприниматься как существительные, хотя это - _глаголы_. Ведь это преобразования, отображения, функции, а значит, - действия. А поскольку это действия, а не сущности, знак равенства следует воспринимать не как тождество объектов, а как выражение эквивалентности направленного действия - как если бы исполнитель обладал всей полнотой информации сразу. Объект же приложения этого действия на письме отсутствует - лишь умозрительно подразумевается, что он появится, когда мы начнем приводить все эти глаголы в движение. \n",
    "<br>Во-вторых, матрица действует на объект слева, и потому цепочку преобразований удобнее читать справа налево.\n",
    "\n",
    "Итак, представьте, что из окружающего нас привычного, \"стандартнобазисного\", мира мы берем некий объект, требующий обработки, трансформации, и мы держим наш объект в руках в конце строки\n",
    "$$\\Large A = V \\cdot \\Lambda \\cdot V^{-1}$$\n",
    "Работать с этим объектом \"как есть\" неудобно - он повернут под углом, и я не уверен, что при обработке смогу правильно соблюсти пропорции, что Мона Лиза, часто используемая в источниках для визуализации собственных векторов, не превратится в шарж на саму себя. Поэтому я выполняю $V^{-1}$, временно перенося объект в некое \"волшебное зазеркалье\". В чем его прелесть? В том, что с каждым направлением, или спектром, осью я могу работать отдельно - трансформировать независимо от остальных, ведь собственные вектора линейно независимы. Лямбда-большая $\\Lambda$, стоящая в центре, по сути, для нас blueprint - таблица, в которой числами указано, как именно требуется трансформировать объект по каждой из существующих независимых осей. Мы просто поосно следуем инструкции - растягиваем или сжимаем.\n",
    "<br>После того, как работа сделана, мы возвращаем наш объект в обычную реальность - поворачиваем его в исходную позу, ровно тем же путем, только в обратном направлении: $V$.\n",
    "<br>Что стоит слева от равенства? $A$. Это таблица цифр, которая привела бы нас к тому же результату трансформации, как если бы мы совсем не знали ничего ни о пространстве, ни о движении в нем, действуя \"вдруг\", по безотчетному наитию. Ведь цифрам все равно - получите ли вы их сразу или цепочкой промежуточных вычислений.\n",
    "\n",
    "Стоит, пожалуй, уточнить метафору поворота. Смена базиса это не поворот объекта как движение в некоем статичном \"мировом пространстве\" - здесь двигается не объект, а скорее глаз наблюдателя, который фиксирует другие, измененные координаты, в других, измененных единицах измерения. Это все, в сущности, условно: можно думать, что объект двигается на фоне неподвижной сетки, а можно, что это сетка, следуя наблюдателю, двигается относительно неподвижного натюрморта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <a id='toc1_1_3_'></a>[__Рекомендательные системы__](#toc0_)\n",
    "* __Content Based Filtering__: feature engineering, feature selection, трудоемкость разметки каждого объекта каждым признаком;\n",
    "* __Collaborative Filtering__: сразу требуется большое количество оценок (_проблема холодного старта_).\n",
    "\n",
    "<br>__Collaborative Filtering, общая идея__ \n",
    "\n",
    "$U_{k \\times n}$, матрица из $n$ пользователей-столбцов, хранящих веса каждого из $k$ параметров индивидуальных предпочтений: больше вес - выше одобрение параметра.\n",
    "\n",
    "$M_{k \\times m}$, матрица из $m$ фильмов-столбцов, хранящих веса каждого из $k$ параметров описания фильмов: больше вес - выше степень соответствия фильма данной категории, параметру и т.д.\n",
    "\n",
    "$A_{n \\times m}$, матрица реальных оценок пользователей: $a_{ij}$ - оценка $i$-ым пользователем $j$-ого фильма.\n",
    "\n",
    "$\\hat{A}_{n \\times m} = U^TM$, матрица предсказанных оценок пользователей: $\\hat{a}_{ij} = \\vec{u_i}^T\\vec{m_j} = \\langle\\vec{u_i},\\vec{m_j}\\rangle$.\n",
    "\n",
    "Квадратичная функция потерь: $L(U, M) = \\sum_{a_{ij} \\ne *} (a_{ij} - \\langle\\vec{u_i},\\vec{m_j}\\rangle)^2$.\n",
    "\n",
    "__Пример__. Найти значение функции потерь, если $A = \\begin{pmatrix}\n",
    "-1 & * & -1\\\\\n",
    "0 & * & *\\\\\n",
    "0 & * & 1\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = np.array([[.5, 0, .25],\n",
    "              [-1, .75, .5]])\n",
    "\n",
    "M = np.array([[-.5, -.25, .5],\n",
    "              [.5, .5, .75]])\n",
    "\n",
    "A_true = np.array([[-1, np.nan, -1],\n",
    "                   [0, np.nan, np.nan],\n",
    "                   [0, np.nan, 1]])\n",
    "\n",
    "A_true = np.ma.array(A_true, mask=np.isnan(A_true))\n",
    "A_pred = U.T @ M\n",
    "\n",
    "((A_true - A_pred)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <a id='toc1_1_4_'></a>[__Сингулярное разложение матрицы (SVD)__](#toc0_)\n",
    "У любой матрицы $A_{m \\times n}$ существует разложение в произведение трех матриц \n",
    "\n",
    "$$\\Large A = U \\Sigma V^*,$$\n",
    "\n",
    "где $U_{m \\times m}$ и $V_{n \\times n}$ - унитарные $(UU^* = \\text{E},\\,VV^* = \\text{E})$, а значит, ортогональные матрицы, состоящие из левых и правых __сингулярных векторов__ соответственно ($V^*$ - сопряженно-транспонированная $V$),\n",
    "\n",
    "$\\Sigma_{m \\times n}$ - диагональная матрица неотрицательных вещественных чисел, называемых __сингулярными числами__ матрицы $A$. Для каждого сингулярного числа $\\sigma$ существуют два единичных вектора $\\vec{u}$ и $\\vec{v}$ такие, что $A\\vec{v} = \\sigma \\vec{u}$ и $A^* \\vec{u} = \\sigma \\vec{v}$, - левый и правый сингулярные векторы соответственно. Левые сингулярные - собственные векторы матрицы $AA^*$, правые - собственные $A^*A$. \n",
    "\n",
    "$\\Sigma$ растягивает $i$-ый базисный вектор в $\\sigma_i$ раз. Если $n \\ne m$, то $\\Sigma$ меняет число координат векторов с $n$ на $m$: либо отрезает несколько последних координат, либо добавляет несколько нулевых координат в конец вектора.\n",
    "\n",
    "Геометрический смысл сингулярного разложения в двумерном случае:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "$A_k = U \\Sigma_k V^T$ - наилучшее по норме Фробениуса приближение матрицы $A$ среди всех матриц рангом не больше $k$. Квадрат нормы Фробениуса, по определению, это сумма квадратов разностей соответствующих элементов матриц. $\\Sigma_k$ получается из $\\Sigma$ заменой нулями всех диагональных элементов, кроме $k$ наибольших (первых, если элементы $\\Sigma$ упорядочены по невозрастанию). \"Сохраненная\" в результате перехода к $\\Sigma_k$ информация называется __объясненной дисперсией__, \"потерянная\" - __необъясненной дисперсией__. См. _Метод главных компонент_ (PCA, _Principal component analysis_).\n",
    "\n",
    "Перед применением SVD признаки необходимо нормировать. См. [__sklearn.preprocessing.StandardScaler__](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn-preprocessing-standardscaler).\n",
    "\n",
    "__Пример__ решения типичных задач. \n",
    "* Даны диагональные элементы $\\Sigma$ и ранг приближающей матрицы. Найти долю утраченной информации.\n",
    "* Даны диагональные элементы $\\Sigma$. Приближение обязано сохранить не менее данной доли информации. Найти ранг приближающей матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varRatio(sigma, rank=None):\n",
    "    sigma = sorted(sigma, reverse=True)\n",
    "    full = np.cumsum(np.square(sigma))\n",
    "    expl = full[rank-1] if rank else full\n",
    "    return expl / full[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoRatio(sigma, *, rank, loss=False):\n",
    "    # loss=False means info retained\n",
    "    if rank not in range(1, len(sigma)+1):\n",
    "        raise ValueError('Incorrect rank')\n",
    "    return abs(loss - varRatio(sigma, rank=rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minRank(sigma, *, thres):\n",
    "    if thres < 0 or thres > 1:\n",
    "        raise ValueError('Incorrect threshold')\n",
    "    return np.argmax(varRatio(sigma) >= thres) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = [8, 5, np.sqrt(11), 3, 1, 1, 0]\n",
    "round(infoRatio(sigma, rank=3, loss=True), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = [9, np.sqrt(37), 4, 2, np.sqrt(3), np.sqrt(3), 1, 0]\n",
    "minRank(sigma, thres=.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "см. [__scipy.linalg.svd__](https://scipy.github.io/devdocs/reference/generated/scipy.linalg.svd.html#scipy-linalg-svd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8c96a75684fb026b06b21d33b3de0343e22c021cb4e229f492bb76a8bcf102e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
